---- Creating Storage Integration _____
use role accountadmin;
create or replace storage integration pacific_retail_project        ---Allows SF to connect to ADLS
    type = external_stage
    storage_provider = 'azure'
    enabled = true
    azure_tenant_id = "a8eec281-aaa3-4dae-ac9b-9a39########"
    storage_allowed_locations = ('azure://ram###.blob.core.windows.net/#######/')

desc storage integration pacific_retail_project;    --- use azure_consent_url for authenticating SF; multi_tenant_app_name ---> to give permissions for SF to modify data.

----- Creating DB  ------
create database if not exists pacific_retail_db;
create schema if not exists bronze;
create schema if not exists silver;
create schema if not exists gold;

----- Since, I am not the admin for my Azure account, I'll be using SAS token to connect SF to Azure and create a staging area -----------
use database pacific_retail_db;
use schema bronze;
create or replace stage adls_stage
    URL = 'azure://#####.blob.core.windows.net/######/'
    CREDENTIALS = (AZURE_SAS_TOKEN = '?sv=2024-11-04&ss=b&srt=sco&sp=rwdlaciytfx&se=2025-09-10T00:36:38Z&st=2025-08-29T16:21:38Z&spr=https&sig=zB2p3Y4D4tdKHuAaSsmRFpT9kx2G73CfnoEiVD%2FR7uc%3D');

---- To see the contents of the stage area -------
list @adls_stage;

------ Creating the file format to load CSV files to staging -------
use database pacific_retail_db;
use schema bronze;
create or replace file format csv_file_format
    type = csv
    field_delimiter = ','
    skip_header = 1
    null_if = ('NULL', 'null', '')
    empty_field_as_null = true
    compression = auto;

select $1, $2, $3, $4, $5, $6 
from @adls_stage/Customer
    (file_format => csv_file_format)
limit 10;

---- Creating "TASK" to automate loading Customers data into the raw table ------
CREATE TABLE IF NOT EXISTS raw_customer (
    customer_id INT,
    name STRING,
    email STRING,
    country STRING,
    customer_type STRING,
    registration_date DATE,
	age INT,
    gender STRING,
    total_purchases INT,
    source_file_name STRING,
    source_file_row_number INT,
    ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP() 
);

create or replace task load_customer_data_task
    warehouse = compute_wh
    SCHEDULE = 'USING CRON 32 13 * * * America/New_York'
AS
    COPY INTO raw_customer(
    customer_id,
        name,
        email,
        country,
        customer_type,
        registration_date,
		age,
		gender,
		total_purchases,
        source_file_name,
        source_file_row_number
     )
     from (
     select 
            $1,
            $2,
            $3,
            $4,
            $5,
            $6::DATE,
			$7,
			$8,
			$9,
            metadata$filename,
            metadata$file_row_number
        FROM @adls_stage/Customer/
     )
FILE_FORMAT = (FORMAT_NAME = 'csv_file_format')
on_error = 'continue'
pattern = '.*[.]csv'

----- Making the task active ----
alter task load_customer_data_task resume;

--------- Loading the JSON files for the products staging ---------
use database pacific_retail_db;
use schema bronze;

create or replace file format json_file_format
    type = JSON
    strip_outer_array = true
    ignore_utf8_errors = true;

select $1 from @adls_stage/Products/
    (file_format => json_file_format)
limit 10;

---- Create the product-raw table ----
CREATE TABLE IF NOT EXISTS raw_product (
    product_id INT,
    name STRING,
    category STRING,
	brand STRING,
    price FLOAT,
	stock_quantity INT,
    rating FLOAT,
    is_active BOOLEAN,
    source_file_name STRING,
    source_file_row_number INT,
    ingestion_timestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);

--- Create task to auto-load products table ---
create or replace task load_product_data_task
    warehouse = compute_wh
    schedule = 'USING CRON 33 13 * * * America/New_York'
AS 
COPY INTO raw_product (
        product_id,
        name,
        category,
		brand,
        price,
		stock_quantity ,
		rating ,
		is_active ,
        source_file_name,
        source_file_row_number
    )
    FROM (
        SELECT
            $1:product_id::INT,
            $1:name::STRING,
            $1:category::STRING,
			$1:brand::STRING,
            $1:price::FLOAT,
            $1:stock_quantity::INT,
			$1:rating::FLOAT,
			$1:is_active::BOOLEAN,
            metadata$filename,
            metadata$file_row_number
        FROM @adls_stage/Products/
    )
    FILE_FORMAT = (FORMAT_NAME = 'json_file_format')
    ON_ERROR = 'CONTINUE'
    PATTERN = '.*[.]json';

    alter task load_product_data_task resume;

---- Creating Parquet file format ----------
create or replace file format parquet_file_format
    type = parquet
    compression = auto
    binary_as_text = false
    trim_space = false;

select * from @adls_stage/Orders/
(file_format => parquet_file_format)
limit 10;

--- create raw_order_table ---
CREATE OR REPLACE TABLE raw_order (
  customer_id INT,
  payment_method STRING,
  product_id INT,
  quantity INT,
  store_type STRING,
  total_amount DOUBLE,
  transaction_date DATE,
  transaction_id STRING,
    source_file_name STRING,
    source_file_row_number INT,
    ingestion_timestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);

--- Creating task to automate order loading ----
CREATE OR REPLACE TASK load_order_data_task
    WAREHOUSE = compute_wh
    SCHEDULE = 'USING CRON 35 13 * * * America/New_York'
AS
    COPY INTO raw_order (
        customer_id,
        payment_method,
        product_id,
        quantity,
        store_type,
        total_amount,
        transaction_date,
        transaction_id,
        source_file_name,
        source_file_row_number
    )
    FROM (
        SELECT
            $1:customer_id::INT,
            $1:payment_method::STRING,
            $1:product_id::INT,
            $1:quantity::INT,
            $1:store_type::STRING,
            $1:total_amount::DOUBLE,
            $1:transaction_date::DATE,
            $1:transaction_id::STRING,
            METADATA$FILENAME,
            METADATA$FILE_ROW_NUMBER
        FROM @adls_stage/Orders/
    )
    FILE_FORMAT = (FORMAT_NAME = 'parquet_file_format')
    ON_ERROR = 'CONTINUE'
    PATTERN = '.*[.]parquet';

-- start task
alter task load_order_data_task resume

